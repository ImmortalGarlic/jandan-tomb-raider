{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled img links\n",
    "with open ('./jandan-links.txt', 'r') as f:\n",
    "    links = f.readlines()\n",
    "links = [x[:-1] for x in links]\n",
    "links = [x for x in links if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pre-process: resize, squarize, to_array\n",
    "import numpy as np\n",
    "import urllib\n",
    "from PIL import Image, ImageOps\n",
    "from io import BytesIO\n",
    "import http\n",
    "\n",
    "def url2img(url):\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url).read()\n",
    "        image = Image.open(BytesIO(response))\n",
    "        # use the first frame if gif\n",
    "        if '.gif' in url: \n",
    "            image.seek(0)\n",
    "        # make sure the image is in RGB mode\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert(mode='RGB')\n",
    "        # center crop\n",
    "        width, height = image.size\n",
    "        new    = min(width, height)\n",
    "        left   = (width - new)/2\n",
    "        top    = (height - new)/2\n",
    "        right  = (width + new)/2\n",
    "        bottom = (height + new)/2\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "        image = image.resize(size=(100,100))\n",
    "        # now convert the image to numpy array\n",
    "        array = np.array(image, dtype=np.float32)\n",
    "        return array\n",
    "    except http.client.IncompleteRead:\n",
    "        pass\n",
    "    except urllib.request.HTTPError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now index:  0\n",
      "Now index:  10\n",
      "Now index:  20\n",
      "Now index:  30\n",
      "Now index:  40\n",
      "Now index:  50\n",
      "Now index:  60\n",
      "Now index:  70\n",
      "Now index:  80\n",
      "Now index:  90\n",
      "Now index:  100\n",
      "Now index:  110\n",
      "Now index:  120\n",
      "Now index:  130\n",
      "Now index:  140\n",
      "Now index:  150\n",
      "Now index:  160\n",
      "Now index:  170\n",
      "Now index:  180\n",
      "Now index:  190\n",
      "Now index:  200\n",
      "Now index:  210\n",
      "Now index:  220\n",
      "Now index:  230\n",
      "Now index:  240\n",
      "Now index:  250\n",
      "Now index:  260\n",
      "Now index:  270\n",
      "Now index:  280\n",
      "Now index:  290\n",
      "Now index:  300\n",
      "Now index:  310\n",
      "Now index:  320\n",
      "Now index:  330\n",
      "Now index:  340\n",
      "Now index:  350\n",
      "Now index:  360\n",
      "Now index:  370\n",
      "Now index:  380\n",
      "Now index:  390\n",
      "Now index:  400\n",
      "Now index:  410\n",
      "Now index:  420\n",
      "Now index:  430\n",
      "Now index:  440\n",
      "Now index:  450\n",
      "Now index:  460\n",
      "Now index:  470\n",
      "Now index:  480\n",
      "Now index:  490\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures as cf\n",
    "\n",
    "test = []\n",
    "def appender(link):\n",
    "    test.append(url2img(link))\n",
    "for idx, l in enumerate(links[:500]):\n",
    "    if idx%10==0:\n",
    "        print ('Now index: ', idx)\n",
    "    appender(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# implementation following notebook by mchablani\n",
    "# https://github.com/mchablani/deep-learning/blob/master/autoencoder/Convolutional_Autoencoder.ipynb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train = test[:4500]\n",
    "validation = test[4500:]\n",
    "\n",
    "learning_rate  = 0.02\n",
    "# Input and target placeholders\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 100,100,3), name=\"input\")\n",
    "targets_ = tf.placeholder(tf.float32, (None, 100,100,3), name=\"target\")\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs=inputs_, filters=32, kernel_size=(10,10), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "# Now 14x14x16\n",
    "conv2 = tf.layers.conv2d(inputs=maxpool1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "# Now 7x7x8\n",
    "conv3 = tf.layers.conv2d(inputs=maxpool2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "encoded = tf.layers.max_pooling2d(conv3, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "# Now 4x4x8\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_images(encoded, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "# Now 7x7x8\n",
    "conv4 = tf.layers.conv2d(inputs=upsample1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "upsample2 = tf.image.resize_images(conv4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "# Now 14x14x8\n",
    "conv5 = tf.layers.conv2d(inputs=upsample2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "upsample3 = tf.image.resize_images(conv5, size=(100,100), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "# Now 28x28x8\n",
    "conv6 = tf.layers.conv2d(inputs=upsample3, filters=32, kernel_size=(10,10), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "\n",
    "logits = tf.layers.conv2d(inputs=conv6, filters=3, kernel_size=(10,10), padding='same', activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "# Pass logits through sigmoid to get reconstructed image\n",
    "decoded = tf.nn.sigmoid(logits)\n",
    "\n",
    "# Pass logits through sigmoid and calculate the cross-entropy loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "\n",
    "# Get cost and define the optimizer\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Training loss: -21.7795\n",
      "Epoch: 1/20... Training loss: -168864048.0000\n",
      "Epoch: 1/20... Training loss: -5337253888.0000\n",
      "Epoch: 1/20... Training loss: -31944015872.0000\n",
      "Epoch: 1/20... Training loss: -153544638464.0000\n",
      "Epoch: 1/20... Training loss: -407476338688.0000\n",
      "Epoch: 1/20... Training loss: -2701666549760.0000\n",
      "Epoch: 1/20... Training loss: -10529154269184.0000\n",
      "Epoch: 1/20... Training loss: -3849255124992.0000\n",
      "Epoch: 1/20... Training loss: -12362748788736.0000\n",
      "Epoch: 1/20... Training loss: -104626997166080.0000\n",
      "Epoch: 1/20... Training loss: -304041271754752.0000\n",
      "Epoch: 1/20... Training loss: -123260796665856.0000\n",
      "Epoch: 1/20... Training loss: -584011197972480.0000\n",
      "Epoch: 1/20... Training loss: -266903511629824.0000\n",
      "Epoch: 1/20... Training loss: -2871117186334720.0000\n",
      "Epoch: 1/20... Training loss: -3450944750616576.0000\n",
      "Epoch: 1/20... Training loss: -816389463998464.0000\n",
      "Epoch: 1/20... Training loss: -4399103878365184.0000\n",
      "Epoch: 1/20... Training loss: -2236987545223168.0000\n",
      "Epoch: 1/20... Training loss: -7746893715079168.0000\n",
      "Epoch: 1/20... Training loss: -15866650720993280.0000\n",
      "Epoch: 1/20... Training loss: -11187044407574528.0000\n",
      "Epoch: 1/20... Training loss: -15386509884522496.0000\n",
      "Epoch: 1/20... Training loss: -27263320671125504.0000\n",
      "Epoch: 1/20... Training loss: -112782164700954624.0000\n",
      "Epoch: 1/20... Training loss: -78063925412757504.0000\n",
      "Epoch: 1/20... Training loss: -394830021329420288.0000\n",
      "Epoch: 1/20... Training loss: -350642092118114304.0000\n",
      "Epoch: 1/20... Training loss: -154132031062671360.0000\n",
      "Epoch: 1/20... Training loss: -226448714707763200.0000\n",
      "Epoch: 1/20... Training loss: -278066645283373056.0000\n",
      "Epoch: 1/20... Training loss: -505874785859796992.0000\n",
      "Epoch: 1/20... Training loss: -311385300758691840.0000\n",
      "Epoch: 1/20... Training loss: -683131728297984000.0000\n",
      "Epoch: 1/20... Training loss: -126357379502571520.0000\n",
      "Epoch: 1/20... Training loss: -2725468598297427968.0000\n",
      "Epoch: 1/20... Training loss: -1876255707343355904.0000\n",
      "Epoch: 1/20... Training loss: -5226872669772709888.0000\n",
      "Epoch: 1/20... Training loss: -13987869280822099968.0000\n",
      "Epoch: 1/20... Training loss: -17568727963837399040.0000\n",
      "Epoch: 1/20... Training loss: -10904615475677757440.0000\n",
      "Epoch: 1/20... Training loss: -3158995862646947840.0000\n",
      "Epoch: 1/20... Training loss: -16545069444238409728.0000\n",
      "Epoch: 1/20... Training loss: -17120065647053635584.0000\n",
      "Epoch: 1/20... Training loss: -29434216546633252864.0000\n",
      "Epoch: 1/20... Training loss: -20087068087793221632.0000\n",
      "Epoch: 1/20... Training loss: -2634831181895434240.0000\n",
      "Epoch: 1/20... Training loss: -44350248463647113216.0000\n",
      "Epoch: 1/20... Training loss: -4591303271871676416.0000\n",
      "Epoch: 1/20... Training loss: -26738642844861333504.0000\n",
      "Epoch: 1/20... Training loss: -64300939726200766464.0000\n",
      "Epoch: 1/20... Training loss: -36450215636634697728.0000\n",
      "Epoch: 1/20... Training loss: -19635069852730785792.0000\n",
      "Epoch: 1/20... Training loss: -125945776019043516416.0000\n",
      "Epoch: 1/20... Training loss: -225620788774239731712.0000\n",
      "Epoch: 1/20... Training loss: -393896926833822138368.0000\n",
      "Epoch: 1/20... Training loss: -238977304184741691392.0000\n",
      "Epoch: 1/20... Training loss: -139702065061311807488.0000\n",
      "Epoch: 1/20... Training loss: -231123079211165679616.0000\n",
      "Epoch: 1/20... Training loss: -498236639660090589184.0000\n",
      "Epoch: 1/20... Training loss: -257564398718713593856.0000\n",
      "Epoch: 1/20... Training loss: -363205176848268918784.0000\n",
      "Epoch: 1/20... Training loss: -1092362655948758581248.0000\n",
      "Epoch: 1/20... Training loss: -509535643359581831168.0000\n",
      "Epoch: 1/20... Training loss: -2364175883387210301440.0000\n",
      "Epoch: 1/20... Training loss: -264129186823978090496.0000\n",
      "Epoch: 1/20... Training loss: -577115181624275763200.0000\n",
      "Epoch: 1/20... Training loss: -781503661994347069440.0000\n",
      "Epoch: 1/20... Training loss: -1636878602039057186816.0000\n",
      "Epoch: 1/20... Training loss: -664310499909702778880.0000\n",
      "Epoch: 1/20... Training loss: -1345394797369035325440.0000\n",
      "Epoch: 1/20... Training loss: -1426061162832170319872.0000\n",
      "Epoch: 1/20... Training loss: -2388500950874545192960.0000\n",
      "Epoch: 1/20... Training loss: -1372555865984218431488.0000\n",
      "Epoch: 1/20... Training loss: -3333132943040343703552.0000\n",
      "Epoch: 1/20... Training loss: -8530742058063237218304.0000\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n",
      "Epoch: 1/20... Training loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c0fdb8bee3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         batch_cost, _ = session.run([cost, opt], feed_dict={inputs_: imgs,\n\u001b[0;32m----> 9\u001b[0;31m                                                          targets_: imgs})\n\u001b[0m\u001b[1;32m     10\u001b[0m         print(\"Epoch: {}/{}...\".format(e+1, epochs),\n\u001b[1;32m     11\u001b[0m               \"Training loss: {:.4f}\".format(batch_cost))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 5\n",
    "session.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(len(train)//batch_size - 1):\n",
    "        batch = train[batch_size*ii: batch_size*(ii+1)]\n",
    "        imgs = batch[0].reshape((-1, 100, 100, 3))\n",
    "        batch_cost, _ = session.run([cost, opt], feed_dict={inputs_: imgs,\n",
    "                                                         targets_: imgs})\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = mnist.test.images[:10]\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: in_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
