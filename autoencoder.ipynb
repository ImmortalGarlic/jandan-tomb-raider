{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled img links\n",
    "with open ('./jandan-links.txt', 'r') as f:\n",
    "    links = f.readlines()\n",
    "links = [x[:-1] for x in links]\n",
    "links = [x for x in links if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pre-process: resize, squarize, to_array\n",
    "import numpy as np\n",
    "import urllib\n",
    "from PIL import Image, ImageOps\n",
    "from io import BytesIO\n",
    "import http\n",
    "\n",
    "def url2img(url):\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url).read()\n",
    "        image = Image.open(BytesIO(response))\n",
    "        # use the first frame if gif\n",
    "        if '.gif' in url: \n",
    "            image.seek(0)\n",
    "        # make sure the image is in RGB mode\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert(mode='RGB')\n",
    "        # center crop\n",
    "        width, height = image.size\n",
    "        new    = min(width, height)\n",
    "        left   = (width - new) // 2\n",
    "        top    = (height - new) // 2\n",
    "        right  = (width + new) // 2\n",
    "        bottom = (height + new) // 2\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "        image = image.resize(size=(100,100))\n",
    "        # now convert the image to numpy array\n",
    "        array = np.array(image, dtype=np.float32)\n",
    "        return array\n",
    "    except http.client.IncompleteRead:\n",
    "        pass\n",
    "    except urllib.request.HTTPError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "img_bag = db.from_sequence(links[:5000])\n",
    "results = img_bag.map(url2img).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# implementation following notebook by mchablani\n",
    "# https://github.com/mchablani/deep-learning/blob/master/autoencoder/Convolutional_Autoencoder.ipynb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train = results[:4000]\n",
    "validation = results[4000:]\n",
    "\n",
    "learning_rate  = 0.01\n",
    "# Input and target placeholders\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 100,100,3), name=\"input\")\n",
    "targets_ = tf.placeholder(tf.float32, (None, 100,100,3), name=\"target\")\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs=inputs_, filters=32, kernel_size=(10,10), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "# Now 14x14x16\n",
    "conv2 = tf.layers.conv2d(inputs=maxpool1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "# Now 7x7x8\n",
    "conv3 = tf.layers.conv2d(inputs=maxpool2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "encoded = tf.layers.max_pooling2d(conv3, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "# Now 4x4x8\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_images(encoded, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "# Now 7x7x8\n",
    "conv4 = tf.layers.conv2d(inputs=upsample1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "upsample2 = tf.image.resize_images(conv4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "# Now 14x14x8\n",
    "conv5 = tf.layers.conv2d(inputs=upsample2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "upsample3 = tf.image.resize_images(conv5, size=(100,100), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "# Now 28x28x8\n",
    "conv6 = tf.layers.conv2d(inputs=upsample3, filters=32, kernel_size=(10,10), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "\n",
    "logits = tf.layers.conv2d(inputs=conv6, filters=3, kernel_size=(10,10), padding='same', activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "# Pass logits through sigmoid to get reconstructed image\n",
    "decoded = tf.nn.sigmoid(logits)\n",
    "\n",
    "# Pass logits through sigmoid and calculate the cross-entropy loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "\n",
    "# Get cost and define the optimizer\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Training loss: -19.8318\n",
      "Epoch: 1/20... Training loss: -8193970.5000\n",
      "Epoch: 1/20... Training loss: -141576848.0000\n",
      "Epoch: 1/20... Training loss: -584238464.0000\n",
      "Epoch: 1/20... Training loss: -2187585792.0000\n",
      "Epoch: 1/20... Training loss: -5083965440.0000\n",
      "Epoch: 1/20... Training loss: -30152822784.0000\n",
      "Epoch: 1/20... Training loss: -113656610816.0000\n",
      "Epoch: 1/20... Training loss: -39158587392.0000\n",
      "Epoch: 1/20... Training loss: -123788886016.0000\n",
      "Epoch: 1/20... Training loss: -1065293316096.0000\n",
      "Epoch: 1/20... Training loss: -2938027900928.0000\n",
      "Epoch: 1/20... Training loss: -1241241223168.0000\n",
      "Epoch: 1/20... Training loss: -6040435294208.0000\n",
      "Epoch: 1/20... Training loss: -2711570874368.0000\n",
      "Epoch: 1/20... Training loss: -29780642627584.0000\n",
      "Epoch: 1/20... Training loss: -36330616651776.0000\n",
      "Epoch: 1/20... Training loss: -8814105460736.0000\n",
      "Epoch: 1/20... Training loss: -47033972948992.0000\n",
      "Epoch: 1/20... Training loss: -24063984533504.0000\n",
      "Epoch: 1/20... Training loss: -86145400569856.0000\n",
      "Epoch: 1/20... Training loss: -176722116870144.0000\n",
      "Epoch: 1/20... Training loss: -125475666001920.0000\n",
      "Epoch: 1/20... Training loss: -175989925609472.0000\n",
      "Epoch: 1/20... Training loss: -323990220439552.0000\n",
      "Epoch: 1/20... Training loss: -1341554241306624.0000\n",
      "Epoch: 1/20... Training loss: -923962288635904.0000\n",
      "Epoch: 1/20... Training loss: -4712151964975104.0000\n",
      "Epoch: 1/20... Training loss: -4231998512037888.0000\n",
      "Epoch: 1/20... Training loss: -1919334314147840.0000\n",
      "Epoch: 1/20... Training loss: -2836664032428032.0000\n",
      "Epoch: 1/20... Training loss: -3445360219389952.0000\n",
      "Epoch: 1/20... Training loss: -6389179390885888.0000\n",
      "Epoch: 1/20... Training loss: -3986507308204032.0000\n",
      "Epoch: 1/20... Training loss: -8922676445839360.0000\n",
      "Epoch: 1/20... Training loss: -1653631933743104.0000\n",
      "Epoch: 1/20... Training loss: -35995115885428736.0000\n",
      "Epoch: 1/20... Training loss: -25269431496081408.0000\n",
      "Epoch: 1/20... Training loss: -70201060064493568.0000\n",
      "Epoch: 1/20... Training loss: -190809917899145216.0000\n",
      "Epoch: 1/20... Training loss: -242401271217127424.0000\n",
      "Epoch: 1/20... Training loss: -149302409057796096.0000\n",
      "Epoch: 1/20... Training loss: -43762632959721472.0000\n",
      "Epoch: 1/20... Training loss: -228579104385925120.0000\n",
      "Epoch: 1/20... Training loss: -242744215765778432.0000\n",
      "Epoch: 1/20... Training loss: -416469990712016896.0000\n",
      "Epoch: 1/20... Training loss: -290091196502507520.0000\n",
      "Epoch: 1/20... Training loss: -37906157288816640.0000\n",
      "Epoch: 1/20... Training loss: -632327281708105728.0000\n",
      "Epoch: 1/20... Training loss: -62132384178372608.0000\n",
      "Epoch: 1/20... Training loss: -385550108831252480.0000\n",
      "Epoch: 1/20... Training loss: -934107695563145216.0000\n",
      "Epoch: 1/20... Training loss: -534119555950182400.0000\n",
      "Epoch: 1/20... Training loss: -301784571383382016.0000\n",
      "Epoch: 1/20... Training loss: -1886477866946789376.0000\n",
      "Epoch: 1/20... Training loss: -3366002016319963136.0000\n",
      "Epoch: 1/20... Training loss: -5944460137547694080.0000\n",
      "Epoch: 1/20... Training loss: -3668541711673458688.0000\n",
      "Epoch: 1/20... Training loss: -2156566438679150592.0000\n",
      "Epoch: 1/20... Training loss: -3598014912699301888.0000\n",
      "Epoch: 1/20... Training loss: -7739592090287341568.0000\n",
      "Epoch: 1/20... Training loss: -3972682845712809984.0000\n",
      "Epoch: 1/20... Training loss: -5654771808977551360.0000\n",
      "Epoch: 1/20... Training loss: -17117247598751645696.0000\n",
      "Epoch: 1/20... Training loss: -8020231438160887808.0000\n",
      "Epoch: 1/20... Training loss: -36891075818907762688.0000\n",
      "Epoch: 1/20... Training loss: -4154524476760915968.0000\n",
      "Epoch: 1/20... Training loss: -9173590548395589632.0000\n",
      "Epoch: 1/20... Training loss: -12489250325326200832.0000\n",
      "Epoch: 1/20... Training loss: -26955193858975072256.0000\n",
      "Epoch: 1/20... Training loss: -10592233227110318080.0000\n",
      "Epoch: 1/20... Training loss: -21700957643321901056.0000\n",
      "Epoch: 1/20... Training loss: -23205415005561290752.0000\n",
      "Epoch: 1/20... Training loss: -38398556838123536384.0000\n",
      "Epoch: 1/20... Training loss: -24530445264287170560.0000\n",
      "Epoch: 1/20... Training loss: -55532840270035943424.0000\n",
      "Epoch: 1/20... Training loss: -147221976927392759808.0000\n",
      "Epoch: 1/20... Training loss: -77509166202487832576.0000\n",
      "Epoch: 1/20... Training loss: -28997945725894524928.0000\n",
      "Epoch: 1/20... Training loss: -128898580466133630976.0000\n",
      "Epoch: 1/20... Training loss: -76011631365456920576.0000\n",
      "Epoch: 1/20... Training loss: -154680157812643856384.0000\n",
      "Epoch: 1/20... Training loss: -146701969500105867264.0000\n",
      "Epoch: 1/20... Training loss: -404803835890755436544.0000\n",
      "Epoch: 1/20... Training loss: -137522525151803998208.0000\n",
      "Epoch: 1/20... Training loss: -288662862453427666944.0000\n",
      "Epoch: 1/20... Training loss: -146116633489943035904.0000\n",
      "Epoch: 1/20... Training loss: -80533861117848453120.0000\n",
      "Epoch: 1/20... Training loss: -306453171697075879936.0000\n",
      "Epoch: 1/20... Training loss: -353455235498732683264.0000\n",
      "Epoch: 1/20... Training loss: -119692255257555894272.0000\n",
      "Epoch: 1/20... Training loss: -213802921139554484224.0000\n",
      "Epoch: 1/20... Training loss: -347758604182574071808.0000\n",
      "Epoch: 1/20... Training loss: -86570074401222426624.0000\n",
      "Epoch: 1/20... Training loss: -1556408424634689519616.0000\n",
      "Epoch: 1/20... Training loss: -339400169564779053056.0000\n",
      "Epoch: 1/20... Training loss: -891296495097547325440.0000\n",
      "Epoch: 1/20... Training loss: -120756520940591906816.0000\n",
      "Epoch: 1/20... Training loss: -635516594954621157376.0000\n",
      "Epoch: 1/20... Training loss: -938682244476832841728.0000\n",
      "Epoch: 1/20... Training loss: -739109378745815793664.0000\n",
      "Epoch: 1/20... Training loss: -1126752776022057287680.0000\n",
      "Epoch: 1/20... Training loss: -982480736015429402624.0000\n",
      "Epoch: 1/20... Training loss: -2217358676272420487168.0000\n",
      "Epoch: 1/20... Training loss: -4653232287939834150912.0000\n",
      "Epoch: 1/20... Training loss: -1999226124633288736768.0000\n",
      "Epoch: 1/20... Training loss: -323642320759214309376.0000\n",
      "Epoch: 1/20... Training loss: -833511582334942445568.0000\n",
      "Epoch: 1/20... Training loss: -5655330492825856901120.0000\n",
      "Epoch: 1/20... Training loss: -3047604726665054257152.0000\n",
      "Epoch: 1/20... Training loss: -1852198655710896062464.0000\n",
      "Epoch: 1/20... Training loss: -454357927753131491328.0000\n",
      "Epoch: 1/20... Training loss: -7850136127326824431616.0000\n",
      "Epoch: 1/20... Training loss: -2980312503982837727232.0000\n",
      "Epoch: 1/20... Training loss: -6387889948863615729664.0000\n",
      "Epoch: 1/20... Training loss: -6186803662551569661952.0000\n",
      "Epoch: 1/20... Training loss: -16143072925416157085696.0000\n",
      "Epoch: 1/20... Training loss: -942474908681776398336.0000\n",
      "Epoch: 1/20... Training loss: -411139.6875\n",
      "Epoch: 1/20... Training loss: -584443.3125\n",
      "Epoch: 1/20... Training loss: -556574.8750\n",
      "Epoch: 1/20... Training loss: -383420.5312\n",
      "Epoch: 1/20... Training loss: -871256.1250\n",
      "Epoch: 1/20... Training loss: -877451.5000\n",
      "Epoch: 1/20... Training loss: -330110.9688\n",
      "Epoch: 1/20... Training loss: -273122.5625\n",
      "Epoch: 1/20... Training loss: -819733.4375\n",
      "Epoch: 1/20... Training loss: -958053.4375\n",
      "Epoch: 1/20... Training loss: -695136.2500\n",
      "Epoch: 1/20... Training loss: -1043375.1875\n",
      "Epoch: 1/20... Training loss: -506549.9062\n",
      "Epoch: 1/20... Training loss: -821531.4375\n",
      "Epoch: 1/20... Training loss: -471321.4375\n",
      "Epoch: 1/20... Training loss: -1030852.3750\n",
      "Epoch: 1/20... Training loss: -478263.2188\n",
      "Epoch: 1/20... Training loss: -1034095.6250\n",
      "Epoch: 1/20... Training loss: -823693.5625\n",
      "Epoch: 1/20... Training loss: -327272.2500\n",
      "Epoch: 1/20... Training loss: -252260.3750\n",
      "Epoch: 1/20... Training loss: -893747.1875\n",
      "Epoch: 1/20... Training loss: -168735.0312\n",
      "Epoch: 1/20... Training loss: -272198.8438\n",
      "Epoch: 1/20... Training loss: -706536.2500\n",
      "Epoch: 1/20... Training loss: -674392.1875\n",
      "Epoch: 1/20... Training loss: -288299.5000\n",
      "Epoch: 1/20... Training loss: -888787.1875\n",
      "Epoch: 1/20... Training loss: -558854.0625\n",
      "Epoch: 1/20... Training loss: -795648.8750\n",
      "Epoch: 1/20... Training loss: -1063806.8750\n",
      "Epoch: 1/20... Training loss: -1088728.3750\n",
      "Epoch: 1/20... Training loss: -1233579.5000\n",
      "Epoch: 1/20... Training loss: -1341316.6250\n",
      "Epoch: 1/20... Training loss: -408597.0312\n",
      "Epoch: 1/20... Training loss: -1050023.5000\n",
      "Epoch: 1/20... Training loss: -1245324.3750\n",
      "Epoch: 1/20... Training loss: -301132.3438\n",
      "Epoch: 1/20... Training loss: -381713.8750\n",
      "Epoch: 1/20... Training loss: -605302.2500\n",
      "Epoch: 1/20... Training loss: -490138.9375\n",
      "Epoch: 1/20... Training loss: -615874.0000\n",
      "Epoch: 1/20... Training loss: -541829.0625\n",
      "Epoch: 1/20... Training loss: -1365866.5000\n",
      "Epoch: 1/20... Training loss: -593847.1875\n",
      "Epoch: 1/20... Training loss: -537348.9375\n",
      "Epoch: 1/20... Training loss: -868502.1875\n",
      "Epoch: 1/20... Training loss: -594669.5625\n",
      "Epoch: 1/20... Training loss: -1317417.7500\n",
      "Epoch: 1/20... Training loss: -811272.2500\n",
      "Epoch: 1/20... Training loss: -1003571.3125\n",
      "Epoch: 1/20... Training loss: -1468411.2500\n",
      "Epoch: 1/20... Training loss: -556521.5625\n",
      "Epoch: 1/20... Training loss: -494139.3125\n",
      "Epoch: 1/20... Training loss: -1014537.1250\n",
      "Epoch: 1/20... Training loss: -642174.4375\n",
      "Epoch: 1/20... Training loss: -616239.8125\n",
      "Epoch: 1/20... Training loss: -705289.6875\n",
      "Epoch: 1/20... Training loss: -521291.7500\n",
      "Epoch: 1/20... Training loss: -1128158.2500\n",
      "Epoch: 1/20... Training loss: -913619.5000\n",
      "Epoch: 1/20... Training loss: -1128794.7500\n",
      "Epoch: 1/20... Training loss: -621793.8125\n",
      "Epoch: 1/20... Training loss: -1289760.7500\n",
      "Epoch: 1/20... Training loss: -950463.3125\n",
      "Epoch: 1/20... Training loss: -1559370.1250\n",
      "Epoch: 1/20... Training loss: -641557.3125\n",
      "Epoch: 1/20... Training loss: -441059.8750\n",
      "Epoch: 1/20... Training loss: -790974.7500\n",
      "Epoch: 1/20... Training loss: -561035.1875\n",
      "Epoch: 1/20... Training loss: -669854.1250\n",
      "Epoch: 1/20... Training loss: -1451303.1250\n",
      "Epoch: 1/20... Training loss: -908064.0625\n",
      "Epoch: 1/20... Training loss: -370277.2188\n",
      "Epoch: 1/20... Training loss: -332105.7812\n",
      "Epoch: 1/20... Training loss: -591336.0000\n",
      "Epoch: 1/20... Training loss: -607192.2500\n",
      "Epoch: 1/20... Training loss: -1520709.8750\n",
      "Epoch: 1/20... Training loss: -520861.5938\n",
      "Epoch: 1/20... Training loss: -972255.5000\n",
      "Epoch: 1/20... Training loss: -894585.5000\n",
      "Epoch: 1/20... Training loss: -1262725.5000\n",
      "Epoch: 1/20... Training loss: -147909.6719\n",
      "Epoch: 1/20... Training loss: -976407.3750\n",
      "Epoch: 1/20... Training loss: -722084.1875\n",
      "Epoch: 1/20... Training loss: -432881.4688\n",
      "Epoch: 1/20... Training loss: -377728.5000\n",
      "Epoch: 1/20... Training loss: -1190967.3750\n",
      "Epoch: 1/20... Training loss: -1114094.5000\n",
      "Epoch: 1/20... Training loss: -668070.6875\n",
      "Epoch: 1/20... Training loss: -693360.8750\n",
      "Epoch: 1/20... Training loss: -665893.1250\n",
      "Epoch: 1/20... Training loss: -897513.7500\n",
      "Epoch: 1/20... Training loss: -663637.1875\n",
      "Epoch: 1/20... Training loss: -888866.1250\n",
      "Epoch: 1/20... Training loss: -895887.3750\n",
      "Epoch: 1/20... Training loss: -952779.5625\n",
      "Epoch: 1/20... Training loss: -718554.4375\n",
      "Epoch: 1/20... Training loss: -1067629.2500\n",
      "Epoch: 1/20... Training loss: -685330.0625\n",
      "Epoch: 1/20... Training loss: -807926.3125\n",
      "Epoch: 1/20... Training loss: -32581.6406\n",
      "Epoch: 1/20... Training loss: -1110524.0000\n",
      "Epoch: 1/20... Training loss: -1048820.6250\n",
      "Epoch: 1/20... Training loss: -1861175.1250\n",
      "Epoch: 1/20... Training loss: -1227681.7500\n",
      "Epoch: 1/20... Training loss: -1035009.6250\n",
      "Epoch: 1/20... Training loss: -804242.9375\n",
      "Epoch: 1/20... Training loss: -1821549.5000\n",
      "Epoch: 1/20... Training loss: -926820.0000\n",
      "Epoch: 1/20... Training loss: -1872593.6250\n",
      "Epoch: 1/20... Training loss: -1077675.3750\n",
      "Epoch: 1/20... Training loss: -1294112.7500\n",
      "Epoch: 1/20... Training loss: -933669.5625\n",
      "Epoch: 1/20... Training loss: -856336.2500\n",
      "Epoch: 1/20... Training loss: -997066.9375\n",
      "Epoch: 1/20... Training loss: -821248.3125\n",
      "Epoch: 1/20... Training loss: -1362632.1250\n",
      "Epoch: 1/20... Training loss: -854922.5625\n",
      "Epoch: 1/20... Training loss: -1926283.7500\n",
      "Epoch: 1/20... Training loss: -818584.5000\n",
      "Epoch: 1/20... Training loss: -924241.6250\n",
      "Epoch: 1/20... Training loss: -1726800.3750\n",
      "Epoch: 1/20... Training loss: -1324431.2500\n",
      "Epoch: 1/20... Training loss: -856239.0625\n",
      "Epoch: 1/20... Training loss: -1014360.2500\n",
      "Epoch: 1/20... Training loss: -1077804.7500\n",
      "Epoch: 1/20... Training loss: -1608167.5000\n",
      "Epoch: 1/20... Training loss: -2080932.6250\n",
      "Epoch: 1/20... Training loss: -432561.5000\n",
      "Epoch: 1/20... Training loss: -1145880.8750\n",
      "Epoch: 1/20... Training loss: -724242.8125\n",
      "Epoch: 1/20... Training loss: -745759.3750\n",
      "Epoch: 1/20... Training loss: -486600.1562\n",
      "Epoch: 1/20... Training loss: -1839639.8750\n",
      "Epoch: 1/20... Training loss: -646024.9375\n",
      "Epoch: 1/20... Training loss: -1856218.1250\n",
      "Epoch: 1/20... Training loss: -2000400.8750\n",
      "Epoch: 1/20... Training loss: -2026365.7500\n",
      "Epoch: 1/20... Training loss: -940054.6875\n",
      "Epoch: 1/20... Training loss: -1084203.8750\n",
      "Epoch: 1/20... Training loss: -584802.5625\n",
      "Epoch: 1/20... Training loss: -631788.6875\n",
      "Epoch: 1/20... Training loss: -870575.3125\n",
      "Epoch: 1/20... Training loss: -1070316.3750\n",
      "Epoch: 1/20... Training loss: -1299298.5000\n",
      "Epoch: 1/20... Training loss: -1646045.0000\n",
      "Epoch: 1/20... Training loss: -1793872.1250\n",
      "Epoch: 1/20... Training loss: -1634036.1250\n",
      "Epoch: 1/20... Training loss: -613532.1250\n",
      "Epoch: 1/20... Training loss: -990226.5625\n",
      "Epoch: 1/20... Training loss: -844465.8125\n",
      "Epoch: 1/20... Training loss: -1061254.3750\n",
      "Epoch: 1/20... Training loss: -1276958.0000\n",
      "Epoch: 1/20... Training loss: -475225.9688\n",
      "Epoch: 1/20... Training loss: -2065290.6250\n",
      "Epoch: 1/20... Training loss: -618462.1875\n",
      "Epoch: 1/20... Training loss: -842312.3750\n",
      "Epoch: 1/20... Training loss: -1971060.8750\n",
      "Epoch: 1/20... Training loss: -1669411.8750\n",
      "Epoch: 1/20... Training loss: -1374139.1250\n",
      "Epoch: 1/20... Training loss: -566217.1250\n",
      "Epoch: 1/20... Training loss: -1948218.1250\n",
      "Epoch: 1/20... Training loss: -984029.1875\n",
      "Epoch: 1/20... Training loss: -678378.5000\n",
      "Epoch: 1/20... Training loss: -942937.4375\n",
      "Epoch: 1/20... Training loss: -1193874.8750\n",
      "Epoch: 1/20... Training loss: -1311606.1250\n",
      "Epoch: 1/20... Training loss: -1333266.0000\n",
      "Epoch: 1/20... Training loss: -1350796.3750\n",
      "Epoch: 1/20... Training loss: -1327759.8750\n",
      "Epoch: 1/20... Training loss: -861840.8125\n",
      "Epoch: 1/20... Training loss: -992282.5000\n",
      "Epoch: 1/20... Training loss: -1245446.7500\n",
      "Epoch: 1/20... Training loss: -1920481.1250\n",
      "Epoch: 1/20... Training loss: -1046241.0625\n",
      "Epoch: 1/20... Training loss: -2181927.0000\n",
      "Epoch: 1/20... Training loss: -1383012.0000\n",
      "Epoch: 1/20... Training loss: -818098.0000\n",
      "Epoch: 1/20... Training loss: -1472925.2500\n",
      "Epoch: 1/20... Training loss: -1164251.3750\n",
      "Epoch: 1/20... Training loss: -1212710.8750\n",
      "Epoch: 1/20... Training loss: -871454.5625\n",
      "Epoch: 1/20... Training loss: -2227079.5000\n",
      "Epoch: 1/20... Training loss: -1232923.8750\n",
      "Epoch: 1/20... Training loss: -1278848.2500\n",
      "Epoch: 1/20... Training loss: -1508061.7500\n",
      "Epoch: 1/20... Training loss: -1363361.5000\n",
      "Epoch: 1/20... Training loss: -1346477.5000\n",
      "Epoch: 1/20... Training loss: -1122791.2500\n",
      "Epoch: 1/20... Training loss: -1688106.3750\n",
      "Epoch: 1/20... Training loss: -1288802.0000\n",
      "Epoch: 1/20... Training loss: -119690.7188\n",
      "Epoch: 1/20... Training loss: -964783.1250\n",
      "Epoch: 1/20... Training loss: -1351687.5000\n",
      "Epoch: 1/20... Training loss: -1043845.0000\n",
      "Epoch: 1/20... Training loss: -1089613.8750\n",
      "Epoch: 1/20... Training loss: -1768757.1250\n",
      "Epoch: 1/20... Training loss: -1193643.3750\n",
      "Epoch: 1/20... Training loss: -915687.8125\n",
      "Epoch: 1/20... Training loss: -1083947.5000\n",
      "Epoch: 1/20... Training loss: -931199.4375\n",
      "Epoch: 1/20... Training loss: -947527.4375\n",
      "Epoch: 1/20... Training loss: -1218941.7500\n",
      "Epoch: 1/20... Training loss: -2470395.2500\n",
      "Epoch: 1/20... Training loss: -2091291.1250\n",
      "Epoch: 1/20... Training loss: -2338088.0000\n",
      "Epoch: 1/20... Training loss: -2593859.2500\n",
      "Epoch: 1/20... Training loss: -1792166.2500\n",
      "Epoch: 1/20... Training loss: -1409683.5000\n",
      "Epoch: 1/20... Training loss: -1416961.0000\n",
      "Epoch: 1/20... Training loss: -2628015.0000\n",
      "Epoch: 1/20... Training loss: -1511595.2500\n",
      "Epoch: 1/20... Training loss: -2242328.2500\n",
      "Epoch: 1/20... Training loss: -2441682.5000\n",
      "Epoch: 1/20... Training loss: -2486818.2500\n",
      "Epoch: 1/20... Training loss: -754617.2500\n",
      "Epoch: 1/20... Training loss: -1695668.8750\n",
      "Epoch: 1/20... Training loss: -2011368.6250\n",
      "Epoch: 1/20... Training loss: -807750.7500\n",
      "Epoch: 1/20... Training loss: -1548915.7500\n",
      "Epoch: 1/20... Training loss: -1074110.1250\n",
      "Epoch: 1/20... Training loss: -1827688.7500\n",
      "Epoch: 1/20... Training loss: -2719252.2500\n",
      "Epoch: 1/20... Training loss: -2496561.7500\n",
      "Epoch: 1/20... Training loss: -983635.8125\n",
      "Epoch: 1/20... Training loss: -319448.4688\n",
      "Epoch: 1/20... Training loss: -1247613.3750\n",
      "Epoch: 1/20... Training loss: -2337347.5000\n",
      "Epoch: 1/20... Training loss: -730163.1250\n",
      "Epoch: 1/20... Training loss: -1273364.2500\n",
      "Epoch: 1/20... Training loss: -1044625.4375\n",
      "Epoch: 1/20... Training loss: -1458441.7500\n",
      "Epoch: 1/20... Training loss: -1681041.0000\n",
      "Epoch: 1/20... Training loss: -2695860.0000\n",
      "Epoch: 1/20... Training loss: -1928236.3750\n",
      "Epoch: 1/20... Training loss: -1006848.0000\n",
      "Epoch: 1/20... Training loss: -1710141.0000\n",
      "Epoch: 1/20... Training loss: -1547858.5000\n",
      "Epoch: 1/20... Training loss: -1724975.0000\n",
      "Epoch: 1/20... Training loss: -1074603.8750\n",
      "Epoch: 1/20... Training loss: -1245661.6250\n",
      "Epoch: 1/20... Training loss: -1072708.7500\n",
      "Epoch: 1/20... Training loss: -1469328.0000\n",
      "Epoch: 1/20... Training loss: -1193483.8750\n",
      "Epoch: 1/20... Training loss: -2850912.0000\n",
      "Epoch: 1/20... Training loss: -1531996.0000\n",
      "Epoch: 1/20... Training loss: -616582.0625\n",
      "Epoch: 1/20... Training loss: -1428439.7500\n",
      "Epoch: 1/20... Training loss: -1654544.7500\n",
      "Epoch: 1/20... Training loss: -1323205.3750\n",
      "Epoch: 1/20... Training loss: -2339017.0000\n",
      "Epoch: 1/20... Training loss: -2410855.7500\n",
      "Epoch: 1/20... Training loss: -1031880.3125\n",
      "Epoch: 1/20... Training loss: -935610.5000\n",
      "Epoch: 1/20... Training loss: -1823932.5000\n",
      "Epoch: 1/20... Training loss: -1470184.2500\n",
      "Epoch: 1/20... Training loss: -1011233.1875\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c0fdb8bee3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         batch_cost, _ = session.run([cost, opt], feed_dict={inputs_: imgs,\n\u001b[1;32m      9\u001b[0m                                                          targets_: imgs})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 5\n",
    "session.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(len(train)//batch_size - 1):\n",
    "        batch = train[batch_size*ii: batch_size*(ii+1)]\n",
    "        imgs = batch[0].reshape((-1, 100, 100, 3))\n",
    "        batch_cost, _ = session.run([cost, opt], feed_dict={inputs_: imgs,\n",
    "                                                         targets_: imgs})\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = mnist.test.images[:10]\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: in_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
